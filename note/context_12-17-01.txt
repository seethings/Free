# INVEST SYSTEM CONTEXT DUMP
# Date: 2025-12-17 00:05:24.734764
# Security: Sensitive files (.env) and temp dirs are EXCLUDED.

ğŸ“¦ PROJECT STRUCTURE (Ignored: .env, temp/, note/, data/)
==================================================
ğŸ“‚ ROOT/
    ğŸ“„ .gitignore  # Gitå¿½ç•¥è§„åˆ™ [æ ¸å¿ƒ]
    ğŸ“„ requirements.txt  # Pythonä¾èµ–æ¸…å• [æ ¸å¿ƒ]
    ğŸ“‚ ui/
    ğŸ“‚ database/
        ğŸ“„ models.py  # SQLAlchemyæ•°æ®åº“æ¨¡å‹(ODS/DWS) [æ ¸å¿ƒ]
    ğŸ“‚ interface/
        ğŸ“„ tushare_client.py  # Tushareæ¥å£å°è£…(å¸¦é‡è¯•) [æ ¸å¿ƒ]
    ğŸ“‚ tools/
        ğŸ“„ db_inspector.py  # æ•°æ®åº“ä½“æ£€å·¥å…· [å·¥å…·]
        ğŸ“„ debug_finance.py
        ğŸ“„ doc_generator.py  # ä¸Šä¸‹æ–‡ç”Ÿæˆå·¥å…· [å·¥å…·]
        ğŸ“„ git_auto.py
    ğŸ“‚ core/
        ğŸ“„ config.py  # å…¨å±€é…ç½®åŠ è½½å™¨ [æ ¸å¿ƒ]
        ğŸ“„ mapping.py  # ä¸­è‹±æ–‡æ˜ å°„å­—å…¸ [æ ¸å¿ƒ]
    ğŸ“‚ engine/
        ğŸ“„ updater.py  # æ•°æ®æ›´æ–°å¼•æ“ [æ ¸å¿ƒ]
==================================================

ğŸ’» CODE CONTENT
==================================================

------------------------------------------------------------
FILE PATH: .gitignore
------------------------------------------------------------
# System
.DS_Store
*.log
__pycache__/

# Python
venv/
*.pyc

# Configuration & Secrets (ç»å¯¹ä¸èƒ½ä¸Šä¼ !)
.env
core/__pycache__

# IDE
.vscode/
.idea/

# Data (å¯é€‰ï¼Œå¦‚æœä¸æƒ³ä¸Šä¼ ä¸´æ—¶ä¸‹è½½çš„æ•°æ®)
data/*
!data/.gitkeep

------------------------------------------------------------
FILE PATH: requirements.txt
------------------------------------------------------------
# Data Source
tushare>=1.2.89

# Data Processing
pandas>=2.0.0
numpy>=1.24.0

# Database (PostgreSQL + SQLAlchemy 2.0)
SQLAlchemy>=2.0.0
psycopg2-binary>=2.9.0

# UI Framework
nicegui>=1.4.0

# Utilities
python-dotenv>=1.0.0
pyyaml>=6.0


------------------------------------------------------------
FILE PATH: database/models.py
------------------------------------------------------------
from sqlalchemy import Column, String, Float, Boolean, DateTime, Integer, Text, PrimaryKeyConstraint
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import DeclarativeBase, sessionmaker
from sqlalchemy import create_engine
from datetime import datetime
from core.config import settings

# 1. æ•°æ®åº“è¿æ¥å¼•æ“
engine = create_engine(settings.DB_URL)

# 2. ä¼šè¯å·¥å‚ (è¿™å°±æ˜¯æŠ¥é”™ç¼ºå¤±çš„éƒ¨åˆ†)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

class Base(DeclarativeBase):
    pass

# --- Meta Data Layer (åŸºç¡€ä¿¡æ¯) ---

class StockBasic(Base):
    """
    è‚¡ç¥¨åŸºç¡€ä¿¡æ¯è¡¨ (PRD 4.1)
    """
    __tablename__ = "stock_basic"

    ts_code = Column(String(20), primary_key=True, comment="TSä»£ç ")
    symbol = Column(String(20), comment="è‚¡ç¥¨ä»£ç ")
    name = Column(String(50), comment="è‚¡ç¥¨åç§°")
    area = Column(String(50), comment="åœ°åŸŸ")
    industry = Column(String(50), comment="æ‰€å±è¡Œä¸š")
    market = Column(String(50), comment="å¸‚åœºç±»å‹")
    list_date = Column(String(8), comment="ä¸Šå¸‚æ—¥æœŸ")
    
    # æ ¸å¿ƒå­—æ®µ: æ ‡è®°æ˜¯å¦ä¸ºä¸­è¯800 (PRD 1.2)
    is_csi800 = Column(Boolean, default=False, index=True, comment="æ˜¯å¦ä¸­è¯800")

class Watchlist(Base):
    """
    ç”¨æˆ·è‡ªé€‰è‚¡æ±  (PRD 1.2)
    """
    __tablename__ = "watchlist"

    ts_code = Column(String(20), primary_key=True)
    name = Column(String(50))
    industry = Column(String(50))
    weight = Column(Float, default=1.0, comment="æƒé‡")
    add_time = Column(DateTime, default=datetime.now)

# --- ODS Layer (åŸå§‹æ•°æ®å±‚ - Store Everything) ---

class ODSMarketDaily(Base):
    """
    æ—¥çº¿è¡Œæƒ… (PRD 2.1)
    """
    __tablename__ = "ods_market_daily"

    ts_code = Column(String(20), primary_key=True)
    trade_date = Column(String(8), primary_key=True, index=True)
    open = Column(Float)
    high = Column(Float)
    low = Column(Float)
    close = Column(Float)
    pre_close = Column(Float)
    change = Column(Float)
    pct_chg = Column(Float)
    vol = Column(Float, comment="æˆäº¤é‡(æ‰‹)")
    amount = Column(Float, comment="æˆäº¤é¢(åƒå…ƒ)")

class ODSAdjFactor(Base):
    """
    å¤æƒå› å­ (PRD 2.1)
    """
    __tablename__ = "ods_adj_factor"

    ts_code = Column(String(20), primary_key=True)
    trade_date = Column(String(8), primary_key=True)
    adj_factor = Column(Float)

class ODSFinanceReport(Base):
    """
    é€šç”¨è´¢åŠ¡æŠ¥è¡¨å­˜å‚¨ (PRD 4.2)
    ç­–ç•¥: JSONB å®½è¡¨æ¨¡å¼
    """
    __tablename__ = "ods_finance_report"

    # å¤åˆä¸»é”®
    ts_code = Column(String(20), primary_key=True)
    end_date = Column(String(8), primary_key=True, comment="æŠ¥å‘ŠæœŸ")
    report_type = Column(String(10), primary_key=True, comment="æŠ¥è¡¨ç±»å‹")
    update_flag = Column(String(5), primary_key=True, default='0', comment="æ›´æ–°æ ‡è®°")
    
    ann_date = Column(String(8), comment="å…¬å‘Šæ—¥æœŸ")
    
    # æ ¸å¿ƒå­—æ®µ
    data = Column(JSONB, comment="åŸå§‹è´¢åŠ¡æ•°æ®JSON")
    category = Column(String(20), index=True, comment="æŠ¥è¡¨ç±»åˆ«")

# --- DWS Layer (æ ‡å‡†æœåŠ¡å±‚ - Strict Logic) ---

class DWSMarketIndicators(Base):
    """
    å¸‚åœºè¡ç”ŸæŒ‡æ ‡è¡¨ (PRD 2.2)
    åŒ…å«: å¤æƒåå‡çº¿, PE/PB/å¸‚å€¼
    """
    __tablename__ = "dws_market_indicators"

    ts_code = Column(String(20), primary_key=True)
    trade_date = Column(String(8), primary_key=True, index=True)
    
    # åŸºç¡€æŒ‡æ ‡ (æ¥è‡ª daily_basic)
    pe_ttm = Column(Float, comment="PE(TTM)")
    pb = Column(Float, comment="å¸‚å‡€ç‡")
    total_mv = Column(Float, comment="æ€»å¸‚å€¼")
    turnover_rate = Column(Float, comment="æ¢æ‰‹ç‡")
    
    # è®¡ç®—æŒ‡æ ‡ (åŸºäº QFQ)
    close_qfq = Column(Float, comment="å‰å¤æƒæ”¶ç›˜ä»·")
    ma_20 = Column(Float, comment="20æ—¥å‡çº¿")
    ma_50 = Column(Float, comment="50æ—¥å‡çº¿")
    ma_120 = Column(Float, comment="120æ—¥å‡çº¿")
    ma_250 = Column(Float, comment="250æ—¥å‡çº¿ (å¹´çº¿)")
    # PRD 3.1 å®¹é”™: è¡Œæ•°<850æ—¶ï¼Œma_850ä¸ºNULL
    ma_850 = Column(Float, comment="850æ—¥å‡çº¿ (ä¸‰å¹´çº¿)")

class DWSFinanceStd(Base):
    """
    æ ‡å‡†åŒ–è´¢åŠ¡å®½è¡¨ (PRD 2.2)
    é€»è¾‘: ä»…å­˜å‚¨ report_type='1' (åˆå¹¶æŠ¥è¡¨) çš„æ¸…æ´—åæ•°æ®
    """
    __tablename__ = "dws_finance_std"

    ts_code = Column(String(20), primary_key=True)
    end_date = Column(String(8), primary_key=True, index=True, comment="æŠ¥å‘ŠæœŸ")
    ann_date = Column(String(8), comment="å…¬å‘Šæ—¥æœŸ")
    
    # æ ¸å¿ƒå­—æ®µ (æ˜ å°„è‡ª Mapping)
    revenue = Column(Float, comment="è¥ä¸šæ”¶å…¥")
    n_income_attr_p = Column(Float, comment="å½’æ¯å‡€åˆ©æ¶¦")
    n_cashflow_act = Column(Float, comment="ç»è¥ç°é‡‘æµ")
    debt_to_assets = Column(Float, comment="èµ„äº§è´Ÿå€ºç‡")
    roe = Column(Float, comment="ROE")
    grossprofit_margin = Column(Float, comment="æ¯›åˆ©ç‡")

# --- å·¥å…·å‡½æ•° ---
def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
    Base.metadata.create_all(bind=engine)

------------------------------------------------------------
FILE PATH: interface/tushare_client.py
------------------------------------------------------------
import tushare as ts
import pandas as pd
import time
from core.config import settings
from functools import wraps

class TushareClient:
    def __init__(self):
        if not settings.TS_TOKEN:
            raise ValueError("Tushare Token is missing in settings")
        
        # åˆå§‹åŒ– Pro æ¥å£ (PRD 1.1)
        self.pro = ts.pro_api(settings.TS_TOKEN)
        print(f"ğŸ“¡ Tushare Client Initialized. Token: {settings.TS_TOKEN[:5]}***")

    def retry_policy(func):
        """
        è£…é¥°å™¨: Tushare å®˜æ–¹å»ºè®®çš„é‡è¯•æœºåˆ¶
        Ref: Tushare PDF [cite: 18]
        """
        @wraps(func)
        def wrapper(*args, **kwargs):
            max_retries = 3
            for i in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    print(f"âš ï¸ API Warning: {e}, Retrying ({i+1}/{max_retries})...")
                    time.sleep(1)
            raise Exception(f"âŒ API Failed after {max_retries} retries.")
        return wrapper

    # --- 1. åŸºç¡€æ•°æ® ---
    
    @retry_policy
    def fetch_stock_basic(self):
        """è·å–å…¨å¸‚åœºè‚¡ç¥¨åˆ—è¡¨ (PRD 2.1)"""
        fields = 'ts_code,symbol,name,area,industry,market,list_date'
        return self.pro.stock_basic(exchange='', list_status='L', fields=fields)

    # --- 2. å¸‚åœºè¡Œæƒ… (Column Storage) ---

    @retry_policy
    def fetch_daily(self, ts_code=None, trade_date=None, start_date=None, end_date=None):
        """
        æ—¥çº¿è¡Œæƒ…
        Ref: Tushare PDF Daily Interface [cite: 252]
        """
        return self.pro.daily(ts_code=ts_code, trade_date=trade_date, 
                              start_date=start_date, end_date=end_date)

    @retry_policy
    def fetch_adj_factor(self, ts_code=None, trade_date=None, start_date=None, end_date=None):
        """å¤æƒå› å­"""
        return self.pro.adj_factor(ts_code=ts_code, trade_date=trade_date, 
                                   start_date=start_date, end_date=end_date)

    # --- 3. è´¢åŠ¡æ•°æ® (JSONB Storage) ---
    # PRD è¦æ±‚: Store Everything, JSONB å®½è¡¨æ¨¡å¼
    # å…³é”®ç‚¹: å¿…é¡»è·å– update_flag ä»¥åŒºåˆ†ä¿®æ­£æŠ¥è¡¨ 

    def _fetch_financial(self, api_func, ts_code, start_date, end_date):
        """é€šç”¨è´¢æŠ¥è·å–é€»è¾‘"""
        # å¼ºåˆ¶æŒ‡å®šå­—æ®µï¼Œç¡®ä¿ update_flag å­˜åœ¨
        fields = 'ts_code,ann_date,f_ann_date,end_date,report_type,comp_type,update_flag'
        
        # æ³¨æ„: Tushare çš„è´¢æŠ¥æ¥å£å­—æ®µéå¸¸å¤šï¼Œè¿™é‡Œæˆ‘ä»¬ä¸æšä¸¾æ‰€æœ‰ metricsï¼Œ
        # è€Œæ˜¯ä¾èµ– Tushare é»˜è®¤è¿”å› (API ä¼šè¿”å›è¯¥æŠ¥è¡¨çš„æ‰€æœ‰å­—æ®µ)ï¼Œ
        # æˆ‘ä»¬åªæ˜¾å¼ç¡®ä¿ meta å­—æ®µå­˜åœ¨ã€‚
        # å®é™…ä¸Šï¼Œå¦‚æœä¸ä¼  fieldsï¼ŒTushare é»˜è®¤è¿”å›æ‰€æœ‰å­—æ®µï¼Œè¿™æ­£ç¬¦åˆæˆ‘ä»¬ JSONB å…¨é‡å­˜å‚¨çš„éœ€æ±‚ã€‚
        # ä½†ä¸ºäº†ç¨³å¥ï¼Œæˆ‘ä»¬åœ¨å¤–éƒ¨è°ƒç”¨æ—¶å¦‚æœä¸ä¼  fieldsï¼Œå®ƒå°±æ˜¯å…¨é‡çš„ã€‚
        
        return api_func(ts_code=ts_code, start_date=start_date, end_date=end_date)

    @retry_policy
    def fetch_income(self, ts_code, start_date, end_date):
        """åˆ©æ¶¦è¡¨"""
        return self.pro.income(ts_code=ts_code, start_date=start_date, end_date=end_date)

    @retry_policy
    def fetch_balancesheet(self, ts_code, start_date, end_date):
        """èµ„äº§è´Ÿå€ºè¡¨"""
        return self.pro.balancesheet(ts_code=ts_code, start_date=start_date, end_date=end_date)

    @retry_policy
    def fetch_cashflow(self, ts_code, start_date, end_date):
        """ç°é‡‘æµé‡è¡¨"""
        return self.pro.cashflow(ts_code=ts_code, start_date=start_date, end_date=end_date)

    @retry_policy
    def fetch_fina_indicator(self, ts_code, start_date, end_date):
        """è´¢åŠ¡æŒ‡æ ‡è¡¨"""
        return self.pro.fina_indicator(ts_code=ts_code, start_date=start_date, end_date=end_date)

# å•ä¾‹æ¨¡å¼
ts_client = TushareClient()

------------------------------------------------------------
FILE PATH: tools/db_inspector.py
------------------------------------------------------------
import sys
import os
import pandas as pd
from sqlalchemy import text, func
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from database.models import SessionLocal, StockBasic, ODSMarketDaily, ODSFinanceReport, DWSMarketIndicators, DWSFinanceStd
from core.config import settings

class DBInspector:
    def __init__(self):
        self.db = SessionLocal()
        print(f"\nğŸ©º === Invest System DB Inspector V1.1 ===")
        print(f"ğŸ“… Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*50)

    def check_counts(self):
        """åŸºç¡€è®¡æ•°æ£€æŸ¥"""
        print("\n[1. Base Counts]")
        
        counts = {
            "Stocks (Total)": self.db.query(StockBasic).count(),
            "ODS Daily Rows": self.db.query(ODSMarketDaily).count(),
            "ODS Finance Rows": self.db.query(ODSFinanceReport).count(), # æ–°å¢
            "DWS Market Rows": self.db.query(DWSMarketIndicators).count(),
            "DWS Finance Rows": self.db.query(DWSFinanceStd).count()
        }
        
        for k, v in counts.items():
            print(f"  - {k:<20}: {v}")

    def inspect_finance_health(self):
        """è´¢åŠ¡æ•°æ®æ·±åº¦æ£€æŸ¥"""
        print("\n[2. Finance Data Health]")
        
        # 1. æ£€æŸ¥ Report Type åˆ†å¸ƒ
        # è¿™èƒ½å¸®æˆ‘ä»¬ç¡®è®¤ Tushare è¿”å›çš„æ˜¯å­—ç¬¦ä¸² '1' è¿˜æ˜¯æ•°å­— 1
        results = self.db.query(
            ODSFinanceReport.report_type, 
            func.count(ODSFinanceReport.ts_code)
        ).group_by(ODSFinanceReport.report_type).all()
        
        if not results:
            print("  âš ï¸ ODS Finance is EMPTY! (Fetch failed)")
        else:
            print(f"  Report Type Dist: {dict(results)}")
            
        # 2. æ£€æŸ¥ JSON ç»“æ„é‡‡æ ·
        if results:
            sample = self.db.query(ODSFinanceReport).first()
            print(f"  Sample Keys (Top 5): {list(sample.data.keys())[:5]}")
            print(f"  Sample Category: {sample.category}")

    def inspect_market_integrity(self, check_date=None):
        print("\n[3. Market Integrity]")
        if not check_date:
            latest = self.db.execute(text("SELECT MAX(trade_date) FROM ods_market_daily")).scalar()
            check_date = latest
        
        if check_date:
            ods_count = self.db.query(ODSMarketDaily).filter(ODSMarketDaily.trade_date == check_date).count()
            dws_ma_count = self.db.execute(text(f"SELECT count(*) FROM dws_market_indicators WHERE trade_date = '{check_date}' AND ma_20 IS NOT NULL")).scalar()
            print(f"  Date: {check_date} | ODS: {ods_count} | DWS(MA): {dws_ma_count}")
        else:
            print("  âš ï¸ No market data.")

    def close(self):
        self.db.close()

if __name__ == "__main__":
    inspector = DBInspector()
    try:
        inspector.check_counts()
        inspector.inspect_finance_health() # æ–°å¢æ£€æŸ¥
        inspector.inspect_market_integrity()
    except Exception as e:
        print(f"âŒ Inspection Failed: {e}")
    finally:
        inspector.close()
    print("\n" + "="*50)

------------------------------------------------------------
FILE PATH: tools/debug_finance.py
------------------------------------------------------------
import sys
import os
import pandas as pd

# Path setup
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from interface.tushare_client import ts_client

def debug_finance_fetch():
    print("ğŸ•µï¸â€â™‚ï¸ === Tushare Finance API Probe ===")
    ts_code = '600000.SH'
    start_date = '20230101'
    end_date = '20240101'
    
    print(f"Target: {ts_code} | Range: {start_date} - {end_date}")
    
    try:
        # 1. ç›´æ¥è°ƒç”¨ Client å°è£…çš„æ–¹æ³•
        print("\n[Attempt 1] Calling fetch_income...")
        df = ts_client.fetch_income(ts_code=ts_code, start_date=start_date, end_date=end_date)
        
        if df is None:
            print("âŒ Result is None!")
        elif df.empty:
            print("âš ï¸ Result is Empty DataFrame!")
        else:
            print(f"âœ… Success! Rows fetched: {len(df)}")
            print("Columns:", df.columns.tolist())
            print("Sample Row 1:\n", df.iloc[0].to_dict())
            
            # æ£€æŸ¥å…³é”®å­—æ®µ
            if 'update_flag' in df.columns:
                print(f"update_flag found: {df['update_flag'].unique()}")
            else:
                print("âš ï¸ Warning: 'update_flag' MISSING in response!")

    except Exception as e:
        print(f"âŒ API Call Failed: {e}")

if __name__ == "__main__":
    debug_finance_fetch()

------------------------------------------------------------
FILE PATH: tools/doc_generator.py
------------------------------------------------------------
import os
import datetime

# --- é…ç½®åŒºåŸŸ ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'note')

# 1. å¿½ç•¥ç›®å½• (æ–°å¢/ç¡®è®¤: temp, note, venv, data)
IGNORE_DIRS = {
    'venv', '__pycache__', '.git', '.idea', '.vscode', 
    'data', 'temp', 'note', 'dist', 'build', 'logs'
}

# 2. å¿½ç•¥æ–‡ä»¶ (æ–°å¢: .env ç»å¯¹ä¸èƒ½ä¸Šä¼ !)
IGNORE_FILES = {
    '.DS_Store', 'poetry.lock', 'package-lock.json', 'LICENSE', 
    '.env', 'requirements.txt.bak'
}

# 3. å…è®¸è¯»å–åç¼€ (ç§»é™¤ .env)
INCLUDE_EXTENSIONS = ('.py', '.txt', '.md', '.gitignore', '.ini', '.yaml', '.yml', '.sh', '.json')

# æ–‡ä»¶æ³¨é‡Šå­—å…¸ (è¾…åŠ© AI ç†è§£æ–‡ä»¶ç”¨é€”)
FILE_META = {
    "requirements.txt": "Pythonä¾èµ–æ¸…å• [æ ¸å¿ƒ]",
    # ".env": "ç¯å¢ƒå˜é‡/å¯†é’¥ [æ ¸å¿ƒ]", # å·²ä»ç”Ÿæˆåˆ—è¡¨ä¸­ç§»é™¤
    ".gitignore": "Gitå¿½ç•¥è§„åˆ™ [æ ¸å¿ƒ]",
    "core/config.py": "å…¨å±€é…ç½®åŠ è½½å™¨ [æ ¸å¿ƒ]",
    "core/mapping.py": "ä¸­è‹±æ–‡æ˜ å°„å­—å…¸ [æ ¸å¿ƒ]",
    "database/models.py": "SQLAlchemyæ•°æ®åº“æ¨¡å‹(ODS/DWS) [æ ¸å¿ƒ]",
    "interface/tushare_client.py": "Tushareæ¥å£å°è£…(å¸¦é‡è¯•) [æ ¸å¿ƒ]",
    "engine/updater.py": "æ•°æ®æ›´æ–°å¼•æ“ [æ ¸å¿ƒ]",
    "tools/doc_generator.py": "ä¸Šä¸‹æ–‡ç”Ÿæˆå·¥å…· [å·¥å…·]",
    "tools/db_inspector.py": "æ•°æ®åº“ä½“æ£€å·¥å…· [å·¥å…·]",
}

def get_today_seq():
    """ç”Ÿæˆ mm-dd-NN æ ¼å¼çš„ç¼–å·"""
    today = datetime.datetime.now().strftime("%m-%d")
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    max_seq = 0
    for f in os.listdir(OUTPUT_DIR):
        if f.startswith(f"context_{today}"):
            try:
                parts = f.split('.')[0].split('-')
                seq = int(parts[-1])
                if seq > max_seq:
                    max_seq = seq
            except:
                pass
    return f"{today}-{max_seq + 1:02d}"

def get_tree_str():
    """ç”Ÿæˆç›®å½•æ ‘å­—ç¬¦ä¸²"""
    lines = []
    lines.append(f"ğŸ“¦ PROJECT STRUCTURE (Ignored: .env, temp/, note/, data/)")
    lines.append(f"{'='*50}")
    
    for root, dirs, files in os.walk(PROJECT_ROOT):
        # è¿‡æ»¤ç›®å½•
        dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
        
        level = root.replace(PROJECT_ROOT, '').count(os.sep)
        indent = ' ' * 4 * level
        folder_name = os.path.basename(root)
        if folder_name == os.path.basename(PROJECT_ROOT):
            folder_name = "ROOT"
            
        lines.append(f"{indent}ğŸ“‚ {folder_name}/")
        
        for f in sorted(files):
            # ä¸¥æ ¼è¿‡æ»¤å¿½ç•¥æ–‡ä»¶
            if f in IGNORE_FILES:
                continue
            
            # åªæœ‰åœ¨ç™½åå•åç¼€é‡Œçš„æ–‡ä»¶æ‰æ˜¾ç¤ºåœ¨æ ‘ä¸­ï¼Œæˆ–è€…ç‰¹å®šçš„é…ç½®æ–‡ä»¶
            # è¿™é‡Œç¨å¾®æ”¾å®½ä¸€ç‚¹ï¼Œè®©æ ‘ç»“æ„å®Œæ•´äº›ï¼Œä½†æ’é™¤æ˜ç¡® ignore çš„
            rel_path = os.path.relpath(os.path.join(root, f), PROJECT_ROOT)
            meta = FILE_META.get(rel_path, "")
            desc = f"  # {meta}" if meta else ""
            
            lines.append(f"{indent}    ğŸ“„ {f}{desc}")
            
    lines.append(f"{'='*50}\n\n")
    return "\n".join(lines)

def generate_context_dump(seq):
    """ç”Ÿæˆå•ä¸€æ•´åˆæ–‡ä»¶"""
    filename = f"context_{seq}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    tree_str = get_tree_str()
    
    with open(filepath, 'w', encoding='utf-8') as outfile:
        # Header
        outfile.write(f"# INVEST SYSTEM CONTEXT DUMP\n")
        outfile.write(f"# Date: {datetime.datetime.now()}\n")
        outfile.write(f"# Security: Sensitive files (.env) and temp dirs are EXCLUDED.\n\n")
        
        # Part 1: Tree
        outfile.write(tree_str)
        
        # Part 2: Content
        outfile.write(f"ğŸ’» CODE CONTENT\n")
        outfile.write(f"{'='*50}\n")
        
        file_count = 0
        for root, dirs, files in os.walk(PROJECT_ROOT):
            dirs[:] = [d for d in dirs if d not in IGNORE_DIRS]
            
            for f in sorted(files):
                # åŒé‡è¿‡æ»¤ï¼šå¿…é¡»ä¸åœ¨å¿½ç•¥åˆ—è¡¨ AND å¿…é¡»åœ¨å…è®¸åç¼€åˆ—è¡¨
                if f in IGNORE_FILES or not f.endswith(INCLUDE_EXTENSIONS):
                    continue
                
                abs_path = os.path.join(root, f)
                rel_path = os.path.relpath(abs_path, PROJECT_ROOT)
                
                outfile.write(f"\n{'-'*60}\n")
                outfile.write(f"FILE PATH: {rel_path}\n")
                outfile.write(f"{'-'*60}\n")
                
                try:
                    with open(abs_path, 'r', encoding='utf-8') as infile:
                        content = infile.read()
                        outfile.write(content)
                        outfile.write("\n")
                        file_count += 1
                except Exception as e:
                    outfile.write(f"[Error reading file: {e}]\n")

    print(f"âœ… å®‰å…¨ä¸Šä¸‹æ–‡å¿«ç…§å·²ç”Ÿæˆ: note/{filename}")
    print(f"ğŸ›¡ï¸ å·²å±è”½ .env åŠä¸´æ—¶ç›®å½• | åŒ…å« {file_count} ä¸ªæ ¸å¿ƒæ–‡ä»¶")

if __name__ == "__main__":
    seq_str = get_today_seq()
    generate_context_dump(seq_str)

------------------------------------------------------------
FILE PATH: tools/git_auto.py
------------------------------------------------------------
import os
import sys
import subprocess
from datetime import datetime

# --- é…ç½® ---
BRANCH = "main"
DOC_GEN_SCRIPT = "tools/doc_generator.py"

def run_cmd(cmd, desc, ignore_error=False):
    """æ‰§è¡Œç³»ç»Ÿå‘½ä»¤"""
    try:
        # capture_output=False è®©å‘½ä»¤è¾“å‡ºç›´æ¥æ˜¾ç¤ºåœ¨å±å¹•ä¸Š
        result = subprocess.run(cmd, shell=True, check=True, text=True, capture_output=False)
        return True
    except subprocess.CalledProcessError as e:
        if not ignore_error:
            print(f"âŒ æ‰§è¡Œå¤±è´¥ [{desc}]: {e}")
        return False

def get_cmd_output(cmd):
    """è·å–å‘½ä»¤è¿”å›çš„æ–‡æœ¬ (é™é»˜æ‰§è¡Œ)"""
    try:
        result = subprocess.run(cmd, shell=True, check=True, text=True, capture_output=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError:
        return ""

def auto_save():
    """åŠŸèƒ½ 1: ä¿å­˜è¿›åº¦"""
    print("\nğŸ’¾ --- æ­£åœ¨ä¿å­˜è¿›åº¦ ---")
    
    # 1. ç”Ÿæˆå¿«ç…§
    if os.path.exists(DOC_GEN_SCRIPT):
        print("1ï¸âƒ£ æ›´æ–° AI ä¸Šä¸‹æ–‡å¿«ç…§...")
        run_cmd(f"python3 {DOC_GEN_SCRIPT}", "ç”Ÿæˆæ–‡æ¡£", ignore_error=True)
    
    # 2. Add
    run_cmd("git add .", "æ·»åŠ æ–‡ä»¶(git add)")
    
    # 3. Commit å‰çš„æ£€æŸ¥
    status_short = get_cmd_output("git status --short")
    if not status_short:
        print("âš ï¸ å½“å‰æ²¡æœ‰æ–‡ä»¶å˜åŠ¨ï¼Œæ— éœ€æäº¤ã€‚")
        # å³ä½¿æ²¡æœ‰å˜åŠ¨ï¼Œå¦‚æœäº‘ç«¯æ»åï¼Œç”¨æˆ·å¯èƒ½åªæƒ³ pushï¼Œæ‰€ä»¥ä¸ç›´æ¥ return
        # ä½†é€šå¸¸ save æ˜¯ä¸ºäº†å­˜æ–°ä¸œè¥¿ã€‚è¿™é‡Œæˆ‘ä»¬ç»§ç»­èµ°ï¼Œæ–¹ä¾¿å•çº¯çš„ push æ“ä½œã€‚
    else:
        print("\nğŸ“ æ£€æµ‹åˆ°ä»¥ä¸‹æ–‡ä»¶å˜åŠ¨ï¼š")
        print("-" * 30)
        print(status_short)
        print("-" * 30)

        # 4. è·å–å¤‡æ³¨
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M')
        default_msg = f"è‡ªåŠ¨å­˜æ¡£: {timestamp}"
        
        print(f"ğŸ’¡ æç¤ºï¼šè¾“å…¥å…·ä½“ä¿®æ”¹å†…å®¹å¯æ–¹ä¾¿æ—¥åå›æº¯")
        user_msg = input(f"âœï¸ æäº¤å¤‡æ³¨ (ç›´æ¥å›è½¦ = '{default_msg}'): ").strip()
        commit_msg = user_msg if user_msg else default_msg
        
        # 5. æ‰§è¡Œæäº¤
        run_cmd(f'git commit -m "{commit_msg}"', "æäº¤ä»£ç (git commit)")
    
    # 6. Push
    print("â˜ï¸ åŒæ­¥åˆ° GitHub...")
    if run_cmd(f"git push origin {BRANCH}", "æ¨é€åˆ°äº‘ç«¯(git push)", ignore_error=True):
        print(f"âœ… ä¿å­˜æˆåŠŸï¼")
    else:
        print("âš ï¸ æ™®é€šæ¨é€å¤±è´¥ï¼è¿™é€šå¸¸æ˜¯å› ä¸ºä½ å›æ»šè¿‡ç‰ˆæœ¬ã€‚")
        print("ğŸ’¡ å»ºè®®ï¼šè¯·å°è¯•ä½¿ç”¨ä¸»èœå•çš„ [4] å¼ºåˆ¶åŒæ­¥ã€‚")

def show_history():
    """åŠŸèƒ½ 2: æŸ¥çœ‹å†å²"""
    print("\nğŸ“œ --- æœ€è¿‘ 10 æ¬¡å­˜æ¡£è®°å½• ---")
    cmd = 'git log -n 10 --pretty=format:"%C(yellow)%h%Creset | %C(cyan)%cd%Creset | %s" --date=format:"%m-%d %H:%M"'
    os.system(cmd) 
    print("\n")

def time_travel():
    """åŠŸèƒ½ 3: æ—¶å…‰å€’æµ"""
    print("\nâ³ --- æ—¶å…‰å€’æµ (å±é™©åŒº) ---")
    print("æ­¤åŠŸèƒ½å¯ä»¥å°†é¡¹ç›®é‡ç½®åˆ°è¿‡å»çš„çŠ¶æ€ã€‚")
    
    confirm = input("ç¡®å®šè¦å›æ»šå—ï¼Ÿ(è¾“å…¥ y ç¡®è®¤): ").lower()
    if confirm != 'y':
        return

    timestamp_str = datetime.now().strftime('%m%d_%H%M%S')
    broken_branch = f"backup/mess_{timestamp_str}"
    
    print(f"\nğŸ›¡ï¸ æ­£åœ¨åˆ›å»ºæ•‘æ´å¤‡ä»½åˆ†æ”¯: {broken_branch} ...")
    run_cmd("git add .", "å¤‡ä»½å½“å‰çŠ¶æ€")
    
    backup_msg = f"[ç³»ç»Ÿ] é‡ç½®å‰è‡ªåŠ¨å¤‡ä»½ (æ—¶é—´: {datetime.now().strftime('%H:%M:%S')})"
    run_cmd(f'git commit -m "{backup_msg}"', "æäº¤å¤‡ä»½", ignore_error=True)
    
    run_cmd(f"git branch {broken_branch}", "åˆ›å»ºå¤‡ä»½åˆ†æ”¯")
    print(f"âœ… å½“å‰çŠ¶æ€å·²å®‰å…¨ä¿å­˜åœ¨åˆ†æ”¯ [{broken_branch}]ã€‚")

    show_history()
    target_hash = input("\nğŸ¯ è¯·è¾“å…¥ä½ è¦å›åˆ°çš„é‚£ä¸ª [Hashç ] (ä¾‹å¦‚ a1b2c3d): ").strip()
    
    if not target_hash:
        print("âŒ æœªè¾“å…¥ Hashï¼Œæ“ä½œå–æ¶ˆã€‚")
        return

    print(f"\nğŸš€ æ­£åœ¨ç©¿è¶Šå› {target_hash} ...")
    if run_cmd(f"git reset --hard {target_hash}", "ç¡¬é‡ç½®(Hard Reset)"):
        print(f"\nâœ… ç©¿è¶ŠæˆåŠŸï¼")
        print("âš ï¸ æ³¨æ„ï¼šä½ éœ€è¦ä½¿ç”¨ä¸»èœå•çš„ [4] å¼ºåˆ¶åŒæ­¥ æ‰èƒ½æŠŠè¿™ä¸ªå˜æ›´æ¨é€åˆ°äº‘ç«¯ã€‚")

def force_sync():
    """åŠŸèƒ½ 4: å¼ºåˆ¶åŒæ­¥ (æ–°å¢)"""
    print("\nâ˜¢ï¸ --- æš´åŠ›åŒæ­¥ (å¼ºåˆ¶è¦†ç›–äº‘ç«¯) ---")
    print("âš ï¸ è­¦å‘Šï¼šè¿™ä¼šå¼ºåˆ¶å°† GitHub ä¸Šçš„ä»£ç æ›¿æ¢ä¸ºä½ ç°åœ¨æœ¬åœ°çš„æ ·å­ã€‚")
    print("âš ï¸ é€‚ç”¨åœºæ™¯ï¼šå½“ä½ æ‰§è¡Œè¿‡ [æ—¶å…‰å€’æµ] åï¼Œæ™®é€šä¿å­˜æŠ¥é”™æ—¶ã€‚")
    
    confirm = input("â“ ç¡®å®šè¦æ‰§è¡Œå—ï¼Ÿ(è¾“å…¥ yes ç¡®è®¤): ").strip()
    if confirm != "yes":
        print("å·²å–æ¶ˆã€‚")
        return

    print(f"ğŸš€ æ­£åœ¨å¼ºåˆ¶æ¨é€ (Force Push) åˆ° {BRANCH} åˆ†æ”¯...")
    if run_cmd(f"git push -f origin {BRANCH}", "å¼ºåˆ¶æ¨é€"):
        print("\nâœ… äº‘ç«¯å·²å¼ºåˆ¶åŒæ­¥ï¼ç°åœ¨ GitHub å’Œä½ æœ¬åœ°å®Œå…¨ä¸€è‡´äº†ã€‚")

def main_menu():
    while True:
        print("\nğŸ¤– === Git æ™ºèƒ½åŠ©ç† (Invest System) ===")
        print("1. ğŸ’¾ ä¿å­˜è¿›åº¦ (Save)  -> æ—¥å¸¸ä½¿ç”¨")
        print("2. ğŸ“œ æŸ¥çœ‹å†å² (Log)   -> çœ‹çœ‹å¹²äº†å•¥")
        print("3. ğŸ”™ æ—¶å…‰å€’æµ (Reset) -> æ•‘å‘½ç”¨çš„")
        print("4. â˜¢ï¸ å¼ºåˆ¶åŒæ­¥ (Force) -> ä¸“æ²»æŠ¥é”™")
        print("0. ğŸšª é€€å‡º (Exit)")
        
        choice = input("ğŸ‘‰ è¯·é€‰æ‹©: ").strip()
        
        if choice == '1':
            auto_save()
        elif choice == '2':
            show_history()
        elif choice == '3':
            time_travel()
        elif choice == '4':
            force_sync()
        elif choice == '0':
            print("Bye! ğŸ‘‹")
            break
        else:
            print("æ— æ•ˆé€‰é¡¹")

if __name__ == "__main__":
    if not os.path.exists(".gitignore"):
        print("âš ï¸ é”™è¯¯ï¼šè¯·åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬ï¼")
    else:
        main_menu()

------------------------------------------------------------
FILE PATH: core/config.py
------------------------------------------------------------
import os
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

class Config:
    # åŸºç¡€é‰´æƒ
    TS_TOKEN = os.getenv("TS_TOKEN")
    DB_URL = os.getenv("DB_URL")
    
    # ç³»ç»Ÿå¸¸é‡ (PRD 1.3)
    START_DATE = "20150101"
    
    # å®Œæ•´æ€§æ£€æŸ¥
    if not TS_TOKEN:
        raise ValueError("âŒ é”™è¯¯: æœªåœ¨ .env ä¸­æ‰¾åˆ° TS_TOKENï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ã€‚")
    if not DB_URL:
        raise ValueError("âŒ é”™è¯¯: æœªåœ¨ .env ä¸­æ‰¾åˆ° DB_URLï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶ã€‚")

settings = Config()

------------------------------------------------------------
FILE PATH: core/mapping.py
------------------------------------------------------------
# å­—æ®µæ˜ å°„å­—å…¸ (Key: DB/API Field, Value: Chinese Name)
# Ref: Tushare Pro å­—æ®µå­—å…¸

FIELD_MAPPING = {
    # --- åŸºç¡€ä¿¡æ¯ ---
    "ts_code": "TSä»£ç ",
    "symbol": "è‚¡ç¥¨ä»£ç ",
    "name": "è‚¡ç¥¨åç§°",
    "industry": "è¡Œä¸š",
    "area": "åœ°åŸŸ",
    "list_date": "ä¸Šå¸‚æ—¥æœŸ",
    "is_csi800": "ä¸­è¯800",

    # --- æ—¥çº¿è¡Œæƒ… (Daily) ---
    "trade_date": "äº¤æ˜“æ—¥æœŸ",
    "open": "å¼€ç›˜ä»·",
    "high": "æœ€é«˜ä»·",
    "low": "æœ€ä½ä»·",
    "close": "æ”¶ç›˜ä»·",
    "pre_close": "æ˜¨æ”¶ä»·",
    "change": "æ¶¨è·Œé¢",
    "pct_chg": "æ¶¨è·Œå¹…(%)",
    "vol": "æˆäº¤é‡(æ‰‹)",
    "amount": "æˆäº¤é¢(åƒå…ƒ)",
    "turnover_rate": "æ¢æ‰‹ç‡(%)",
    "pe_ttm": "å¸‚ç›ˆç‡(TTM)",
    "pb": "å¸‚å‡€ç‡",
    "total_mv": "æ€»å¸‚å€¼(ä¸‡)",
    "circ_mv": "æµé€šå¸‚å€¼(ä¸‡)",

    # --- è´¢åŠ¡æ ¸å¿ƒæŒ‡æ ‡ (Indicator/Income/Cashflow) ---
    "end_date": "æŠ¥å‘ŠæœŸ",
    "ann_date": "å…¬å‘Šæ—¥æœŸ",
    "report_type": "æŠ¥è¡¨ç±»å‹",
    
    # åˆ©æ¶¦è¡¨ (Income) [cite: 526]
    "total_revenue": "è¥ä¸šæ€»æ”¶å…¥",
    "revenue": "è¥ä¸šæ”¶å…¥",
    "n_income_attr_p": "å½’æ¯å‡€åˆ©æ¶¦",  # æ ¸å¿ƒå­—æ®µ
    "operate_profit": "è¥ä¸šåˆ©æ¶¦",
    
    # èµ„äº§è´Ÿå€ºè¡¨ (Balance Sheet) [cite: 549]
    "total_assets": "èµ„äº§æ€»è®¡",
    "total_liab": "è´Ÿå€ºåˆè®¡",
    "total_hldr_eqy_exc_min_int": "è‚¡ä¸œæƒç›Šåˆè®¡(ä¸å«å°‘æ•°è‚¡ä¸œ)", # å½’æ¯æƒç›Š
    "money_cap": "è´§å¸èµ„é‡‘",
    
    # ç°é‡‘æµé‡è¡¨ (Cashflow) [cite: 587]
    "n_cashflow_act": "ç»è¥æ´»åŠ¨ç°é‡‘å‡€æµé‡",
    "n_cashflow_inv_act": "æŠ•èµ„æ´»åŠ¨ç°é‡‘å‡€æµé‡",
    "n_cashflow_fnc_act": "ç­¹èµ„æ´»åŠ¨ç°é‡‘å‡€æµé‡",

    # è´¢åŠ¡æŒ‡æ ‡ (Indicator) [cite: 648]
    "roe": "ROE",
    "roe_dt": "ROE(æ‰£é)",
    "grossprofit_margin": "æ¯›åˆ©ç‡",
    "netprofit_margin": "å‡€åˆ©ç‡",
    "debt_to_assets": "èµ„äº§è´Ÿå€ºç‡",
    "current_ratio": "æµåŠ¨æ¯”ç‡",
}

------------------------------------------------------------
FILE PATH: engine/updater.py
------------------------------------------------------------
import pandas as pd
from sqlalchemy.dialects.postgresql import insert
from interface.tushare_client import ts_client
from database.models import SessionLocal, StockBasic, ODSMarketDaily, ODSAdjFactor, ODSFinanceReport, DWSMarketIndicators, DWSFinanceStd
from core.config import settings
from datetime import datetime

class DataUpdater:
    def __init__(self):
        self.db = SessionLocal()

    def sync_stock_list(self):
        """
        å…¨é‡åŒæ­¥è‚¡ç¥¨åˆ—è¡¨ï¼Œå¹¶æ ‡è®°ä¸­è¯800æˆåˆ†è‚¡ (PRD 3.1)
        """
        print("ğŸ”„ å¼€å§‹åŒæ­¥è‚¡ç¥¨åŸºç¡€åˆ—è¡¨...")
        
        # 1. æ‹‰å–å…¨å¸‚åœºè‚¡ç¥¨ (Tushare API)
        df_basics = ts_client.fetch_stock_basic()
        if df_basics.empty:
            print("âš ï¸ æœªè·å–åˆ°è‚¡ç¥¨åˆ—è¡¨ï¼Œä»»åŠ¡ç»ˆæ­¢")
            return

        # 2. æ‹‰å–ä¸­è¯800æˆåˆ†è‚¡ (ç”¨äºæ ‡è®°æ ¸å¿ƒèµ„äº§)
        # 000906.SH æ˜¯ä¸­è¯800æŒ‡æ•°ä»£ç 
        # æ³¨æ„: index_weight æ¥å£éœ€è¦ 2000 ç§¯åˆ† 
        try:
            # è·å–æœ€æ–°ä¸€ä¸ªæœˆçš„æˆåˆ†è‚¡ï¼ˆè¿™é‡Œç®€åŒ–é€»è¾‘ï¼Œå–ä¸Šä¸ªæœˆçš„æˆåˆ†ï¼‰
            # å®é™…ç”Ÿäº§ä¸­å¯èƒ½éœ€è¦åŠ¨æ€è®¡ç®—æ—¥æœŸï¼Œè¿™é‡Œæš‚å–æœ€è¿‘çš„é€»è¾‘
            # Tushare Pro çš„ index_weight é€šå¸¸æŒ‰æœˆæ›´æ–°
            now_str = datetime.now().strftime("%Y%m%d")
            # å°è¯•æ‹‰å–æœ€æ–°çš„æˆåˆ†
            df_csi800 = ts_client.pro.index_weight(index_code='000906.SH', start_date='20240101', end_date=now_str)
            
            # å¦‚æœæ²¡å–åˆ°ï¼ˆæ¯”å¦‚å¹´åˆè¿˜æ²¡æ›´æ–°ï¼‰ï¼Œå¯ä»¥å°è¯•å–å»å¹´çš„ï¼Œè¿™é‡Œåšç®€å•å®¹é”™
            if df_csi800.empty:
                 print("âš ï¸ è­¦å‘Š: æœªè·å–åˆ°ä¸­è¯800æˆåˆ†è‚¡ï¼Œå°†è·³è¿‡æ ‡è®°æ­¥éª¤")
                 csi800_set = set()
            else:
                 # å–æœ€æ–°æ—¥æœŸçš„æˆåˆ†
                 latest_date = df_csi800['trade_date'].max()
                 df_latest = df_csi800[df_csi800['trade_date'] == latest_date]
                 csi800_set = set(df_latest['con_code'].tolist())
                 print(f"âœ… è·å–åˆ°ä¸­è¯800æˆåˆ†è‚¡ ({latest_date}): {len(csi800_set)} åª")

        except Exception as e:
            print(f"âš ï¸ ä¸­è¯800æ¥å£è°ƒç”¨å¤±è´¥: {e}")
            csi800_set = set()

        # 3. æ•°æ®å¤„ç†ä¸æ ‡è®°
        # é»˜è®¤å…¨éƒ¨ False
        df_basics['is_csi800'] = False
        # å¦‚æœä»£ç åœ¨ csi800_set ä¸­ï¼Œè®¾ä¸º True
        if csi800_set:
            df_basics.loc[df_basics['ts_code'].isin(csi800_set), 'is_csi800'] = True

        # 4. å†™å…¥æ•°æ®åº“ (Upsert æ¨¡å¼)
        # ä½¿ç”¨ SQLAlchemy Core çš„ bulk insert æ•ˆç‡è¾ƒé«˜ï¼Œæˆ–è€…é€è¡Œ merge
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºæ¸…æ™°ï¼Œä½¿ç”¨ pandas to_sql çš„æ›¿ä»£æ–¹æ¡ˆæˆ– ORM å¾ªç¯
        # è€ƒè™‘åˆ°åªæœ‰ 5000 æ¡æ•°æ®ï¼ŒORM æ•ˆç‡å¯æ¥å—
        
        count = 0
        for _, row in df_basics.iterrows():
            stock = StockBasic(
                ts_code=row['ts_code'],
                symbol=row['symbol'],
                name=row['name'],
                area=row['area'],
                industry=row['industry'],
                market=row['market'],
                list_date=row['list_date'],
                is_csi800=row['is_csi800']
            )
            self.db.merge(stock) # merge ä¼šæ ¹æ®ä¸»é”®è‡ªåŠ¨ insert æˆ– update
            count += 1
            
        self.db.commit()
        print(f"âœ… è‚¡ç¥¨åˆ—è¡¨åŒæ­¥å®Œæˆ! å…±å¤„ç†: {count} åª, å…¶ä¸­ä¸­è¯800: {len(csi800_set)} åª")

    def sync_daily_market(self, start_date: str, end_date: str):
        """
        S3 åœºæ™¯: æ—¥çº¿è¡Œæƒ…å¢é‡æ›´æ–° (ODSå±‚)
        """
        print(f"ğŸ“ˆ å¼€å§‹åŒæ­¥æ—¥çº¿è¡Œæƒ… ({start_date} - {end_date})...")
        
        # 1. è·å–æ—¥çº¿ (å…¨å¸‚åœº)
        # ç­–ç•¥: æŒ‰æ—¥æœŸå¾ªç¯æ‹‰å–ï¼Œæ¯å¤©çº¦ 5000 æ¡ï¼Œé€‚åˆ WideCore æ¨¡å¼
        # [cite_start]Tushare daily æ¥å£æ”¯æŒå•æ—¥å…¨é‡ [cite: 1515]
        
        dates = pd.date_range(start=start_date, end=end_date).strftime('%Y%m%d').tolist()
        
        for trade_date in dates:
            try:
                # 1.1 æ‹‰å–è¡Œæƒ…
                df_daily = ts_client.fetch_daily(trade_date=trade_date)
                if df_daily.empty:
                    print(f"  - {trade_date}: æ— æ•°æ® (ä¼‘å¸‚?)")
                    continue
                
                # 1.2 æ‹‰å–å¤æƒå› å­
                df_adj = ts_client.fetch_adj_factor(trade_date=trade_date)
                
                # 1.3 å…¥åº“ ODSMarketDaily
                # ä½¿ç”¨ to_dict è½¬æ¢ï¼Œåˆ©ç”¨ SQLAlchemy çš„ bulk_insert_mappings (éœ€ Core æ¨¡å¼) 
                # æˆ–å¾ªç¯ ORM merge (ç®€å•ä½†æ…¢)ã€‚é‰´äºæ¯æ—¥ä»… 5000 æ¡ï¼ŒORM merge å°šå¯ï¼Œ
                # ä½†ä¸ºæ€§èƒ½æ¨è bulk insert (è¿™é‡Œç®€åŒ–æ¼”ç¤ºä½¿ç”¨ merge é€»è¾‘çš„å˜ä½“)
                
                daily_objs = []
                for _, row in df_daily.iterrows():
                    daily_objs.append({
                        "ts_code": row['ts_code'],
                        "trade_date": row['trade_date'],
                        "open": row['open'],
                        "high": row['high'],
                        "low": row['low'],
                        "close": row['close'],
                        "pre_close": row['pre_close'],
                        "change": row['change'],
                        "pct_chg": row['pct_chg'],
                        "vol": row['vol'],
                        "amount": row['amount']
                    })
                
                # æ‰¹é‡æ’å…¥ (ä½¿ç”¨ Core çš„ insert..on_conflict_do_update ä¼šæ›´ä¼˜ï¼Œè¿™é‡Œç”¨ ORM é€ä¸ªæ·»åŠ æ¼”ç¤º)
                # å®é™…ç”Ÿäº§å»ºè®®: self.db.execute(insert(ODSMarketDaily).values(daily_objs).on_conflict_do_nothing())
                # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ä¿æŒç®€å•é€»è¾‘ï¼š
                for obj in daily_objs:
                    self.db.merge(ODSMarketDaily(**obj))
                
                # 1.4 å…¥åº“ ODSAdjFactor
                if not df_adj.empty:
                    for _, row in df_adj.iterrows():
                        self.db.merge(ODSAdjFactor(
                            ts_code=row['ts_code'],
                            trade_date=row['trade_date'],
                            adj_factor=row['adj_factor']
                        ))
                
                self.db.commit()
                print(f"  âœ… {trade_date}: è¡Œæƒ…å…¥åº“å®Œæˆ (Stocks: {len(df_daily)})")
                
            except Exception as e:
                self.db.rollback()
                print(f"  âŒ {trade_date}: å¤„ç†å¤±è´¥ - {e}")

    def sync_financial_report(self, ts_code: str, start_date: str = None, end_date: str = None):
        """
        S2/S4 åœºæ™¯: è´¢æŠ¥æ•°æ®æ›´æ–° (ODSå±‚ - JSONB)
        [FIXED V2]: å¼ºåŠ›ä¿®å¤ NaN -> Noneï¼Œå…¼å®¹ PostgreSQL JSONB
        """
        print(f"ğŸ’° å¼€å§‹åŒæ­¥è´¢æŠ¥: {ts_code}...")
        
        tasks = {
            "income": (ts_client.fetch_income, "income"),
            "balancesheet": (ts_client.fetch_balancesheet, "balance"),
            "cashflow": (ts_client.fetch_cashflow, "cashflow"),
            "fina_indicator": (ts_client.fetch_fina_indicator, "indicator")
        }
        
        for name, (api_func, category) in tasks.items():
            try:
                # 1. æ‹‰å–æ•°æ®
                df = api_func(ts_code=ts_code, start_date=start_date, end_date=end_date)
                if df.empty:
                    continue
                
                # 2. [å…³é”®ä¿®å¤] æ•°æ®æ¸…æ´—
                # å…ˆè½¬ä¸º object ç±»å‹ï¼Œé˜²æ­¢ pandas å°† None è‡ªåŠ¨å›æ»šä¸º NaN
                # ç„¶åå°†æ‰€æœ‰ NaN æ›¿æ¢ä¸º None (JSON null)
                df = df.astype(object).where(pd.notnull(df), None)
                
                # 3. è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
                records = df.to_dict('records')
                
                # 4. é€æ¡å…¥åº“ (Merge)
                # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ bulk_insert ä¼šæ›´å¿«ï¼Œä½†ä¸ºäº†æ¼”ç¤º update_flag é€»è¾‘ä¿æŒå¾ªç¯
                # ç”Ÿäº§ç¯å¢ƒå»ºè®®ä¼˜åŒ–ä¸º bulk_insert_mappings
                for record in records:
                    pk_data = {
                        "ts_code": record.get("ts_code"),
                        "end_date": record.get("end_date"),
                        "report_type": record.get("report_type", '1'), 
                        "update_flag": record.get("update_flag", '0'),
                        "ann_date": record.get("ann_date"),
                        "category": category,
                        "data": record # record ä¸­çš„ NaN ç°åœ¨æ˜¯ None äº†
                    }
                    
                    self.db.merge(ODSFinanceReport(**pk_data))
                
                self.db.commit()
                print(f"  - {name}: {len(records)} æ¡è®°å½•")
                
            except Exception as e:
                self.db.rollback()
                print(f"  âš ï¸ {name} åŒæ­¥å¤±è´¥: {e}")

    def process_market_dws(self, ts_code: str):
        """
        DWS æ ¸å¿ƒé€»è¾‘: è®¡ç®—å¤æƒä»·æ ¼ä¸å‡çº¿ (PRD 2.2)
        è§¦å‘æ—¶æœº: å•åªè‚¡ç¥¨ ODS è¡Œæƒ…æ›´æ–°å
        """
        print(f"ğŸ§® è®¡ç®— DWS æŒ‡æ ‡: {ts_code}...")
        
        # 1. è¯»å– ODS æ•°æ® (Raw Price + Adj Factor)
        # ä½¿ç”¨ pandas read_sql ç®€åŒ–å¤„ç†
        query_daily = f"SELECT * FROM ods_market_daily WHERE ts_code = '{ts_code}' ORDER BY trade_date"
        query_adj = f"SELECT trade_date, adj_factor FROM ods_adj_factor WHERE ts_code = '{ts_code}' ORDER BY trade_date"
        
        df_daily = pd.read_sql(query_daily, self.db.bind)
        df_adj = pd.read_sql(query_adj, self.db.bind)
        
        if df_daily.empty or df_adj.empty:
            print("  âš ï¸ æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®¡ç®—")
            return

        # 2. åˆå¹¶å¤æƒå› å­
        df = pd.merge(df_daily, df_adj, on='trade_date', how='left')
        # å¡«å……ç¼ºå¤±å› å­ (å‘å‰å¡«å……)
        df['adj_factor'] = df['adj_factor'].ffill()
        
        # 3. è®¡ç®—å‰å¤æƒä»·æ ¼ (QFQ)
        # å…¬å¼: P_qfq = P_raw * (Factor_curr / Factor_latest) 
        latest_factor = df['adj_factor'].iloc[-1]
        df['close_qfq'] = df['close'] * (df['adj_factor'] / latest_factor)
        
        # 4. è®¡ç®—å‡çº¿ (MA)
        # PRD 2.2: MA20, MA50, MA120, MA250, MA850
        ma_list = [20, 50, 120, 250, 850]
        for ma in ma_list:
            col_name = f'ma_{ma}'
            # min_periods=ma ç¡®ä¿æ•°æ®ä¸å¤Ÿæ—¶ä¸º NaN (None)
            df[col_name] = df['close_qfq'].rolling(window=ma, min_periods=ma).mean()
            
        # 5. å‡†å¤‡å…¥åº“æ•°æ® (DWSMarketIndicators)
        dws_records = []
        for _, row in df.iterrows():
            # åŸºç¡€æŒ‡æ ‡è½¬æ¢
            record = {
                "ts_code": row['ts_code'],
                "trade_date": row['trade_date'],
                "close_qfq": row['close_qfq'],
                "ma_20": row['ma_20'] if pd.notna(row['ma_20']) else None,
                "ma_50": row['ma_50'] if pd.notna(row['ma_50']) else None,
                "ma_120": row['ma_120'] if pd.notna(row['ma_120']) else None,
                "ma_250": row['ma_250'] if pd.notna(row['ma_250']) else None,
                "ma_850": row['ma_850'] if pd.notna(row['ma_850']) else None,
                # é€ä¼  ODS åŸºç¡€å­—æ®µ (ç”¨äºé›·è¾¾ç­›é€‰)
                "turnover_rate": None, # éœ€ä» daily_basic è¡¥å……ï¼Œæ­¤å¤„æš‚ç•™ç©ºæˆ–åç»­ Join
                "pe_ttm": None,        # åŒä¸Š
                "pb": None,            # åŒä¸Š
                "total_mv": None       # åŒä¸Š
            }
            dws_records.append(record)
            
        # 6. æ‰¹é‡å…¥åº“ (Upsert)
        for r in dws_records:
            self.db.merge(DWSMarketIndicators(**r))
            
        self.db.commit()
        print(f"  âœ… DWS è®¡ç®—å®Œæˆ: {len(dws_records)} æ¡å‡çº¿æ•°æ®")

    def process_finance_dws(self, ts_code: str):
        """
        DWS æ ¸å¿ƒé€»è¾‘: æ ‡å‡†åŒ–è´¢åŠ¡å®½è¡¨æ¸…æ´— (PRD 2.2)
        è§„åˆ™: ä»…æå– report_type='1' (åˆå¹¶æŠ¥è¡¨)
        """
        print(f"ğŸ§¹ æ¸…æ´—è´¢åŠ¡æ•°æ®: {ts_code}...")
        
        # 1. æå–æ‰€æœ‰ç±»å‹çš„ JSONB æ•°æ®
        # è·å–è¯¥è‚¡ç¥¨æ‰€æœ‰ report_type='1' çš„è®°å½•
        reports = self.db.query(ODSFinanceReport).filter(
            ODSFinanceReport.ts_code == ts_code,
            ODSFinanceReport.report_type == '1'
        ).all()
        
        # æŒ‰ end_date èšåˆæ•°æ®
        # ç»“æ„: { '20231231': { 'revenue': 100, 'roe': 5... } }
        merged_data = {}
        
        for r in reports:
            if r.end_date not in merged_data:
                merged_data[r.end_date] = {"ann_date": r.ann_date}
            
            # å°† JSONB ä¸­çš„æ•°æ®æ‰“å¹³åˆå¹¶
            # æ˜ å°„å…³ç³»å‚è€ƒ core/mapping.py
            # å®é™…ç”Ÿäº§å»ºè®®ä¸¥æ ¼æŒ‰ Mapping æå–ï¼Œè¿™é‡Œåšè‡ªåŠ¨æ˜ å°„
            raw_dict = r.data
            target_fields = [
                'revenue', 'n_income_attr_p', 'n_cashflow_act', 
                'debt_to_assets', 'roe', 'grossprofit_margin'
            ]
            
            for field in target_fields:
                if field in raw_dict:
                    merged_data[r.end_date][field] = raw_dict[field]

        # 2. å…¥åº“ DWSFinanceStd
        for end_date, metrics in merged_data.items():
            dws_obj = DWSFinanceStd(
                ts_code=ts_code,
                end_date=end_date,
                ann_date=metrics.get('ann_date'),
                revenue=metrics.get('revenue'),
                n_income_attr_p=metrics.get('n_income_attr_p'),
                n_cashflow_act=metrics.get('n_cashflow_act'),
                debt_to_assets=metrics.get('debt_to_assets'),
                roe=metrics.get('roe'),
                grossprofit_margin=metrics.get('grossprofit_margin')
            )
            self.db.merge(dws_obj)
            
        self.db.commit()
        print(f"  âœ… è´¢åŠ¡æ¸…æ´—å®Œæˆ: {len(merged_data)} ä¸ªæŠ¥å‘ŠæœŸ")

    def close(self):
        self.db.close()

# å¿«æ·å…¥å£
if __name__ == "__main__":
    updater = DataUpdater()
    updater.sync_stock_list()
    updater.close()
